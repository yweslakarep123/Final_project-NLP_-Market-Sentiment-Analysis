{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "abEo59_WT7Wy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_google_news_url(keywords, days):\n",
        "    url = \"https://news.google.com/search?q=\"\n",
        "    for k in keywords:\n",
        "        url += k\n",
        "        url += \"+\"\n",
        "    url += \"when:\"\n",
        "    url += str(days)\n",
        "    url += \"d\"\n",
        "    return url"
      ],
      "metadata": {
        "id": "4HZ7ZtKnUnWd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_news(symbols, urls):\n",
        "    tickers = symbols\n",
        "    news_urls = urls\n",
        "    all_news = []\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    for i in range(len(tickers)):\n",
        "        ticker = tickers[i]\n",
        "        news_url = news_urls[i]\n",
        "\n",
        "        try:\n",
        "            page = requests.get(news_url, headers=headers).text\n",
        "            soup = BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "            # Debug: print URL untuk cek\n",
        "            print(f\"Fetching: {news_url[:80]}...\")\n",
        "\n",
        "            # Coba beberapa selector (Google sering berubah)\n",
        "            # Selector 1: artikel news\n",
        "            articles = soup.select('article')\n",
        "\n",
        "            for article in articles:\n",
        "                # Cari judul\n",
        "                title_elem = article.select_one('a[href*=\"./articles/\"]') or \\\n",
        "                             article.select_one('h3') or \\\n",
        "                             article.select_one('h4') or \\\n",
        "                             article.select_one('[class*=\"title\"]')\n",
        "\n",
        "                # Cari waktu\n",
        "                time_elem = article.select_one('time') or \\\n",
        "                            article.select_one('[datetime]')\n",
        "\n",
        "                if title_elem:\n",
        "                    title = title_elem.get_text(strip=True)\n",
        "                    timedate = time_elem.get('datetime', 'N/A') if time_elem else 'N/A'\n",
        "\n",
        "                    if title:  # hanya tambah jika ada judul\n",
        "                        all_news.append((ticker, timedate, title))\n",
        "\n",
        "            print(f\"  Found {len([n for n in all_news if n[0]==ticker])} articles for {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {ticker}: {e}\")\n",
        "\n",
        "    news = pd.DataFrame(all_news, columns=['Ticker', 'Date', 'Headline'])\n",
        "\n",
        "    # Hapus duplikat jika ada\n",
        "    news = news.drop_duplicates(subset=['Headline'])\n",
        "\n",
        "    return news\n",
        "\n",
        "\n",
        "# ============ ALTERNATIF: Gunakan Google News RSS ============\n",
        "def get_google_news_url_rss(keywords, days=7):\n",
        "    \"\"\"Generate Google News RSS URL\"\"\"\n",
        "    if isinstance(keywords, list):\n",
        "        query = \"+\".join(keywords)\n",
        "    else:\n",
        "        query = keywords.replace(\" \", \"+\")\n",
        "\n",
        "    return f\"https://news.google.com/rss/search?q={query}+when:{days}d&hl=id&gl=ID&ceid=ID:id\"\n",
        "\n",
        "\n",
        "def get_news_rss(symbols, urls):\n",
        "    \"\"\"Scrape news menggunakan RSS (lebih stabil)\"\"\"\n",
        "    all_news = []\n",
        "\n",
        "    for i in range(len(symbols)):\n",
        "        ticker = symbols[i]\n",
        "        news_url = urls[i]\n",
        "\n",
        "        try:\n",
        "            page = requests.get(news_url).text\n",
        "            soup = BeautifulSoup(page, 'xml')\n",
        "\n",
        "            items = soup.find_all('item')\n",
        "\n",
        "            for item in items:\n",
        "                title = item.title.text if item.title else ''\n",
        "                pubdate = item.pubDate.text if item.pubDate else 'N/A'\n",
        "\n",
        "                if title:\n",
        "                    all_news.append((ticker, pubdate, title))\n",
        "\n",
        "            print(f\"{len(items)} news from {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {ticker}: {e}\")\n",
        "\n",
        "    news = pd.DataFrame(all_news, columns=['Ticker', 'Date', 'Headline'])\n",
        "    return news"
      ],
      "metadata": {
        "id": "Y8NlfZE2Updy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdQ_V7RdWVKy",
        "outputId": "c070daab-e657-4c7c-e025-9dbcf004d001"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQqUPTonXaJS",
        "outputId": "bf71ca12-7ed0-49ca-9054-e7bfe4b615ef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saham-saham populer Indonesia (IDX)\n",
        "symbols = [\"BBCA\", \"BBRI\", \"TLKM\", \"ASII\", \"BMRI\"]\n",
        "\n",
        "# Keyword bisa berupa list of words atau string\n",
        "# Angka 7 = berita dari 7 hari terakhir\n",
        "news_url = [\n",
        "    get_google_news_url([\"Bank BCA saham\"], 7),\n",
        "    get_google_news_url([\"Bank BRI saham\"], 7),\n",
        "    get_google_news_url([\"Telkom Indonesia saham\"], 7),\n",
        "    get_google_news_url([\"Astra International saham\"], 7),\n",
        "    get_google_news_url([\"Bank Mandiri saham\"], 7)\n",
        "]\n",
        "\n",
        "# Pastikan jumlah symbols = jumlah news_url\n",
        "print(f\"Symbols: {len(symbols)}, URLs: {len(news_url)}\")\n",
        "\n",
        "All_News = get_news(symbols, news_url)\n",
        "print(All_News)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzs5y346VeB-",
        "outputId": "478ccb42-6c36-4d11-c3cd-30214a7c0d82"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbols: 5, URLs: 5\n",
            "Fetching: https://news.google.com/search?q=Bank BCA saham+when:7d...\n",
            "  Found 0 articles for BBCA\n",
            "Fetching: https://news.google.com/search?q=Bank BRI saham+when:7d...\n",
            "  Found 0 articles for BBRI\n",
            "Fetching: https://news.google.com/search?q=Telkom Indonesia saham+when:7d...\n",
            "  Found 0 articles for TLKM\n",
            "Fetching: https://news.google.com/search?q=Astra International saham+when:7d...\n",
            "  Found 0 articles for ASII\n",
            "Fetching: https://news.google.com/search?q=Bank Mandiri saham+when:7d...\n",
            "  Found 0 articles for BMRI\n",
            "Empty DataFrame\n",
            "Columns: [Ticker, Date, Headline]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_google_news_url_rss(keywords, days=7):\n",
        "    \"\"\"Generate Google News RSS URL\"\"\"\n",
        "    if isinstance(keywords, list):\n",
        "        query = \"+\".join(keywords)\n",
        "    else:\n",
        "        query = keywords.replace(\" \", \"+\")\n",
        "\n",
        "    # RSS URL dengan bahasa Indonesia\n",
        "    return f\"https://news.google.com/rss/search?q={query}+when:{days}d&hl=id&gl=ID&ceid=ID:id\"\n",
        "\n",
        "\n",
        "def get_news_rss(symbols, urls):\n",
        "    \"\"\"Scrape news menggunakan RSS (lebih stabil)\"\"\"\n",
        "    all_news = []\n",
        "\n",
        "    for i in range(len(symbols)):\n",
        "        ticker = symbols[i]\n",
        "        news_url = urls[i]\n",
        "\n",
        "        try:\n",
        "            response = requests.get(news_url)\n",
        "            soup = BeautifulSoup(response.content, 'xml')\n",
        "\n",
        "            items = soup.find_all('item')\n",
        "\n",
        "            for item in items:\n",
        "                title = item.title.text if item.title else ''\n",
        "                pubdate = item.pubDate.text if item.pubDate else 'N/A'\n",
        "                link = item.link.text if item.link else ''\n",
        "\n",
        "                if title:\n",
        "                    all_news.append((ticker, pubdate, title, link))\n",
        "\n",
        "            print(f\"{len(items)} news from {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {ticker}: {e}\")\n",
        "\n",
        "    news = pd.DataFrame(all_news, columns=['Ticker', 'Date', 'Headline', 'Link'])\n",
        "    return news\n",
        "\n",
        "\n",
        "# ========== JALANKAN ==========\n",
        "symbols = [\"BBCA\", \"BBRI\", \"TLKM\", \"ASII\", \"BMRI\"]\n",
        "\n",
        "news_url = [\n",
        "    get_google_news_url_rss(\"Bank BCA saham\", 7),\n",
        "    get_google_news_url_rss(\"Bank BRI saham\", 7),\n",
        "    get_google_news_url_rss(\"Telkom Indonesia saham\", 7),\n",
        "    get_google_news_url_rss(\"Astra International saham\", 7),\n",
        "    get_google_news_url_rss(\"Bank Mandiri saham\", 7)\n",
        "]\n",
        "\n",
        "print(\"URLs generated:\")\n",
        "for i, url in enumerate(news_url):\n",
        "    print(f\"  {symbols[i]}: {url}\")\n",
        "print()\n",
        "\n",
        "All_News = get_news_rss(symbols, news_url)\n",
        "print(f\"\\nTotal: {len(All_News)} articles\")\n",
        "print(All_News)\n",
        "\n",
        "# ========== SIMPAN KE CSV ==========\n",
        "filename = \"indonesia_market_news.csv\"\n",
        "All_News.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n✅ Data tersimpan ke: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUIjPc4xVvdF",
        "outputId": "935e34f0-2edb-4411-8294-20a715c31d07"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URLs generated:\n",
            "  BBCA: https://news.google.com/rss/search?q=Bank+BCA+saham+when:7d&hl=id&gl=ID&ceid=ID:id\n",
            "  BBRI: https://news.google.com/rss/search?q=Bank+BRI+saham+when:7d&hl=id&gl=ID&ceid=ID:id\n",
            "  TLKM: https://news.google.com/rss/search?q=Telkom+Indonesia+saham+when:7d&hl=id&gl=ID&ceid=ID:id\n",
            "  ASII: https://news.google.com/rss/search?q=Astra+International+saham+when:7d&hl=id&gl=ID&ceid=ID:id\n",
            "  BMRI: https://news.google.com/rss/search?q=Bank+Mandiri+saham+when:7d&hl=id&gl=ID&ceid=ID:id\n",
            "\n",
            "44 news from BBCA\n",
            "42 news from BBRI\n",
            "49 news from TLKM\n",
            "39 news from ASII\n",
            "57 news from BMRI\n",
            "\n",
            "Total: 231 articles\n",
            "    Ticker                           Date  \\\n",
            "0     BBCA  Wed, 19 Nov 2025 23:30:00 GMT   \n",
            "1     BBCA  Thu, 20 Nov 2025 01:05:51 GMT   \n",
            "2     BBCA  Mon, 17 Nov 2025 22:17:03 GMT   \n",
            "3     BBCA  Sun, 16 Nov 2025 06:19:00 GMT   \n",
            "4     BBCA  Thu, 20 Nov 2025 06:09:00 GMT   \n",
            "..     ...                            ...   \n",
            "226   BMRI  Tue, 18 Nov 2025 10:19:02 GMT   \n",
            "227   BMRI  Fri, 14 Nov 2025 10:18:45 GMT   \n",
            "228   BMRI  Mon, 17 Nov 2025 03:26:50 GMT   \n",
            "229   BMRI  Mon, 17 Nov 2025 13:37:13 GMT   \n",
            "230   BMRI  Thu, 20 Nov 2025 02:32:32 GMT   \n",
            "\n",
            "                                              Headline  \\\n",
            "0    Curi Start Borong Saham BCA (BBCA) Jelang Hila...   \n",
            "1           Ada yang Galau di Saham BBCA - investor.id   \n",
            "2    Kinerja Saham Blue Chip Ini Tertekan Jelang Ak...   \n",
            "3    Saham Konglomerasi Dominan, Regulator Akan Tin...   \n",
            "4    Cara Daftar BCA Sekuritas: Panduan Lengkap Mem...   \n",
            "..                                                 ...   \n",
            "226  BBNI Jadi Primadona! Saham Big Banks Ditutup B...   \n",
            "227  Sentimen Campur Saham Big Banks: Akhir Pekan C...   \n",
            "228  Indeks Bisnis-27 Dibuka Menguat, Saham Bank Ju...   \n",
            "229  Bank Mandiri Buyback Saham Rp1,17 Triliun, Ana...   \n",
            "230  IHSG Menguat! SGRO, BMRI, Dan BREN Pimpin Kena...   \n",
            "\n",
            "                                                  Link  \n",
            "0    https://news.google.com/rss/articles/CBMiuwFBV...  \n",
            "1    https://news.google.com/rss/articles/CBMib0FVX...  \n",
            "2    https://news.google.com/rss/articles/CBMirAFBV...  \n",
            "3    https://news.google.com/rss/articles/CBMiakFVX...  \n",
            "4    https://news.google.com/rss/articles/CBMiugFBV...  \n",
            "..                                                 ...  \n",
            "226  https://news.google.com/rss/articles/CBMilAFBV...  \n",
            "227  https://news.google.com/rss/articles/CBMijwFBV...  \n",
            "228  https://news.google.com/rss/articles/CBMilgFBV...  \n",
            "229  https://news.google.com/rss/articles/CBMi0AFBV...  \n",
            "230  https://news.google.com/rss/articles/CBMinAFBV...  \n",
            "\n",
            "[231 rows x 4 columns]\n",
            "\n",
            "✅ Data tersimpan ke: indonesia_market_news.csv\n"
          ]
        }
      ]
    }
  ]
}